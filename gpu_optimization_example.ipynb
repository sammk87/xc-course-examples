{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Optimization Example\n",
    "This notebook demonstrates techniques for optimizing GPU performance using CUDA and `pycuda`.\n",
    "- **Memory Optimization Techniques:**\n",
    "  - Reducing memory bandwidth usage.\n",
    "  - Effective use of shared memory.\n",
    "- **Thread and Block Management:**\n",
    "  - Maximizing occupancy.\n",
    "  - Avoiding divergence.\n",
    "- **Profiling Tools:** NVIDIA Nsight.\n",
    "\n",
    "**Steps:**\n",
    "1. Install necessary libraries.\n",
    "2. Define and run CUDA kernel using `pycuda`.\n",
    "3. Analyze optimization techniques in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "install-pycuda"
   },
   "outputs": [],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-optimization-code"
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "from pycuda.compiler import SourceModule\n",
    "import time\n",
    "\n",
    "# Define the CUDA kernel\n",
    "kernel_code = \"\"\"\n",
    "__global__ void matrix_add_optimized(float *a, float *b, float *c, int N) {\n",
    "    // Shared memory for blocks\n",
    "    __shared__ float shared_a[256];\n",
    "    __shared__ float shared_b[256];\n",
    "    \n",
    "    int tid = threadIdx.x;  // Thread ID within the block\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;  // Global thread ID\n",
    "    \n",
    "    if (gid < N) {\n",
    "        // Load data into shared memory\n",
    "        shared_a[tid] = a[gid];\n",
    "        shared_b[tid] = b[gid];\n",
    "        __syncthreads();  // Ensure all threads in the block load data\n",
    "        \n",
    "        // Perform addition\n",
    "        c[gid] = shared_a[tid] + shared_b[tid];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Compile the CUDA kernel\n",
    "mod = SourceModule(kernel_code)\n",
    "matrix_add_optimized = mod.get_function(\"matrix_add_optimized\")\n",
    "\n",
    "# Define matrix size\n",
    "N = 1024 * 1024\n",
    "a = np.random.rand(N).astype(np.float32)\n",
    "b = np.random.rand(N).astype(np.float32)\n",
    "c = np.zeros_like(a)\n",
    "\n",
    "# Allocate device memory\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "\n",
    "# Copy data to device\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "# Define block and grid sizes\n",
    "block_size = 256\n",
    "grid_size = (N + block_size - 1) // block_size\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Launch the kernel\n",
    "matrix_add_optimized(\n",
    "    a_gpu, b_gpu, c_gpu,\n",
    "    np.int32(N),\n",
    "    block=(block_size, 1, 1), grid=(grid_size, 1)\n",
    ")\n",
    "\n",
    "# Copy result back to host\n",
    "cuda.memcpy_dtoh(c, c_gpu)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Verify results\n",
    "print(\"First 10 elements of c:\", c[:10])\n",
    "print(\"Execution Time:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Clean up\n",
    "a_gpu.free()\n",
    "b_gpu.free()\n",
    "c_gpu.free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Optimizations\n",
    "#### **Memory Optimization Techniques**\n",
    "1. **Reducing Memory Bandwidth Usage:**\n",
    "   - Data is loaded into shared memory (`shared_a` and `shared_b`) from global memory. Shared memory is much faster than global memory.\n",
    "   - Threads within a block use shared memory to perform the addition, reducing the number of global memory accesses.\n",
    "\n",
    "2. **Effective Use of Shared Memory:**\n",
    "   - Each thread loads a part of the data into shared memory. The `__syncthreads()` ensures all threads have loaded their respective data before proceeding.\n",
    "\n",
    "#### **Thread and Block Management**\n",
    "1. **Maximizing Occupancy:**\n",
    "   - The grid and block sizes are calculated dynamically. `block_size` is chosen as 256, which is a multiple of the warp size (32), ensuring optimal GPU utilization.\n",
    "\n",
    "2. **Avoiding Divergence:**\n",
    "   - The kernel avoids divergent branching by using a single condition (`if gid < N`) that all threads in the block evaluate consistently.\n",
    "\n",
    "#### **Profiling Tools**\n",
    "To profile this code:\n",
    "1. Download the script (`matrix_add.cu`) or run equivalent CUDA code on your local machine.\n",
    "2. Use **NVIDIA Nsight Systems** or **NVIDIA Nsight Compute** to analyze kernel execution time, memory throughput, and thread utilization.\n",
    "\n",
    "### Profiling with NVIDIA Nsight\n",
    "1. Install Nsight tools from NVIDIA.\n",
    "2. Profile the execution by running:\n",
    "   ```bash\n",
    "   nv-nsight-cu-cli ./matrix_add.cu\n",
    "   ```\n",
    "3. Look for metrics like:\n",
    "   - Memory access patterns.\n",
    "   - Warp divergence.\n",
    "   - Shared memory usage."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gpu_optimization_example.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 }
}
