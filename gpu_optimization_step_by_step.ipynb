{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Optimization Step-by-Step\n",
    "This notebook demonstrates GPU optimization techniques using CUDA and `pycuda`. We'll compare an **unoptimized implementation** with step-by-step optimizations and measure the improvements in performance.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand the impact of unoptimized global memory access.\n",
    "- Apply memory optimization techniques (e.g., shared memory).\n",
    "- Optimize thread and block management to maximize GPU occupancy.\n",
    "- Compare execution times and understand performance improvements.\n",
    "\n",
    "## Steps\n",
    "1. Install required libraries.\n",
    "2. Implement an unoptimized GPU kernel.\n",
    "3. Add memory optimizations.\n",
    "4. Optimize thread and block configurations.\n",
    "5. Measure and compare execution times for each version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Unoptimized Kernel\n",
    "This kernel performs matrix addition with **unoptimized global memory access**. Each thread directly accesses global memory for all operations, which is inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "from pycuda.compiler import SourceModule\n",
    "import time\n",
    "\n",
    "# Define unoptimized kernel\n",
    "unoptimized_kernel = \"\"\"\n",
    "__global__ void matrix_add_unoptimized(float *a, float *b, float *c, int N) {\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;  // Global thread ID\n",
    "    if (gid < N) {\n",
    "        c[gid] = a[gid] + b[gid];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Compile the unoptimized kernel\n",
    "mod = SourceModule(unoptimized_kernel)\n",
    "matrix_add_unoptimized = mod.get_function(\"matrix_add_unoptimized\")\n",
    "\n",
    "# Initialize data\n",
    "N = 1024 * 1024\n",
    "a = np.random.rand(N).astype(np.float32)\n",
    "b = np.random.rand(N).astype(np.float32)\n",
    "c = np.zeros_like(a)\n",
    "\n",
    "# Allocate device memory\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "b_gpu = cuda.mem_alloc(b.nbytes)\n",
    "c_gpu = cuda.mem_alloc(c.nbytes)\n",
    "\n",
    "# Copy data to device\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "cuda.memcpy_htod(b_gpu, b)\n",
    "\n",
    "# Define block and grid sizes\n",
    "block_size = 256\n",
    "grid_size = (N + block_size - 1) // block_size\n",
    "\n",
    "# Measure execution time for unoptimized kernel\n",
    "start_time = time.time()\n",
    "matrix_add_unoptimized(\n",
    "    a_gpu, b_gpu, c_gpu,\n",
    "    np.int32(N),\n",
    "    block=(block_size, 1, 1), grid=(grid_size, 1)\n",
    ")\n",
    "cuda.memcpy_dtoh(c, c_gpu)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time (Unoptimized):\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Optimizing Memory Access\n",
    "In this step, we use **shared memory** to reduce the number of accesses to global memory, which improves memory bandwidth utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimized kernel with shared memory\n",
    "optimized_memory_kernel = \"\"\"\n",
    "__global__ void matrix_add_shared_memory(float *a, float *b, float *c, int N) {\n",
    "    __shared__ float shared_a[256];\n",
    "    __shared__ float shared_b[256];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (gid < N) {\n",
    "        // Load data into shared memory\n",
    "        shared_a[tid] = a[gid];\n",
    "        shared_b[tid] = b[gid];\n",
    "        __syncthreads();\n",
    "        \n",
    "        // Perform addition using shared memory\n",
    "        c[gid] = shared_a[tid] + shared_b[tid];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Compile the optimized memory kernel\n",
    "mod = SourceModule(optimized_memory_kernel)\n",
    "matrix_add_shared_memory = mod.get_function(\"matrix_add_shared_memory\")\n",
    "\n",
    "# Measure execution time for shared memory kernel\n",
    "start_time = time.time()\n",
    "matrix_add_shared_memory(\n",
    "    a_gpu, b_gpu, c_gpu,\n",
    "    np.int32(N),\n",
    "    block=(block_size, 1, 1), grid=(grid_size, 1)\n",
    ")\n",
    "cuda.memcpy_dtoh(c, c_gpu)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time (Shared Memory):\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Optimizing Thread and Block Management\n",
    "Now, we'll tune the block size and grid size to maximize GPU occupancy and avoid divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust block size for optimal GPU occupancy\n",
    "block_size = 512  # Increase block size for better GPU utilization\n",
    "grid_size = (N + block_size - 1) // block_size\n",
    "\n",
    "# Measure execution time with optimized block size\n",
    "start_time = time.time()\n",
    "matrix_add_shared_memory(\n",
    "    a_gpu, b_gpu, c_gpu,\n",
    "    np.int32(N),\n",
    "    block=(block_size, 1, 1), grid=(grid_size, 1)\n",
    ")\n",
    "cuda.memcpy_dtoh(c, c_gpu)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time (Optimized Block Size):\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "We started with an unoptimized kernel and progressively applied optimization techniques:\n",
    "\n",
    "- **Unoptimized Kernel:** Direct global memory access.\n",
    "- **Memory Optimization:** Used shared memory to reduce global memory access latency.\n",
    "- **Thread/Block Optimization:** Tuned block size for better GPU occupancy.\n",
    "\n",
    "Compare the execution times to see the improvements."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gpu_optimization_step_by_step.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 }
}
